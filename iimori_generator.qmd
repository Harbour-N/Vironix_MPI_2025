---
title: VAE GAN framework
description: VAE with distribution analysis for each category
authors:
  - name: Nicholas Harbour, Nipuni de Silva
format:
  html:
    embed-resources: true
    code-fold: true
    number-sections: true
    toc: true
    toc-depth: 3
    date: now
    date-modified: last-modified
    date-format: "MMMM DD, YYYY, HH:mm:ss"
jupyter: python3
---


```{python}

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Hyperparameters
batch_size = 128
latent_dim = 100
learning_rate = 2e-4
num_epochs = 100

cols = ['gender', 'age', 'BMI', 'Hb', 'Alb', 'Cr', 'UPCR', 'eGFR(0M)', 'eGFR(6M)', 'eGFR(12M)', 'eGFR(18M)', 'eGFR(24M)', 'eGFR(30M)', 'eGFR(36M)','eGFR(last visit)']

# Load data
df_ful = pd.read_excel('iimori-ckd_processed.xlsx', sheet_name='minimally_processed')
# filter only keep columns: Gender, Age, BMI, Hb, Alb, Cr, uPCR, and eGFR time series
df_ful = df_ful[cols]
# Drop any rows that contain NaNs in these selected columns
df_ful = df_ful.dropna()

df_in = pd.read_excel('iimori-ckd_processed.xlsx', sheet_name='eGFR_increase')
df_in = df_in[cols]
df_in = df_in.dropna()

df_de = pd.read_excel('iimori-ckd_processed.xlsx', sheet_name='eGFR_decrease')
df_de = df_de[cols]
df_de = df_de.dropna()



input_dim = df.shape[1]

class CKDDataset(Dataset):
    def __init__(self, dataframe):
        self.X_raw = torch.tensor(dataframe.values, dtype=torch.float32)

    def __len__(self):
        return len(self.X_raw)

    def __getitem__(self, idx):
        x = self.X_raw[idx]
        return x

dataset = CKDDataset(df)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)



```

# Generator
class Generator(nn.Module):
    def __init__(self, noise_dim, output_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(noise_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, output_dim),
        )

    def forward(self, z):
        return self.net(z)

# Discriminator
class Discriminator(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.net(x)

G = Generator(latent_dim, input_dim).to(device)
D = Discriminator(input_dim).to(device)

opt_G = optim.Adam(G.parameters(), lr=learning_rate, betas=(0.5, 0.999))
opt_D = optim.Adam(D.parameters(), lr=learning_rate, betas=(0.5, 0.999))

criterion = nn.BCELoss()

# Masked BCE Loss to avoid NaNs
def masked_bce_loss(predictions, targets):
    mask = ~torch.isnan(targets)
    masked_preds = predictions[mask]
    masked_targets = targets[mask]
    if masked_targets.numel() == 0:
        return torch.tensor(0.0, device=device, requires_grad=True)
    return F.binary_cross_entropy(masked_preds, masked_targets)

# Training Loop
G_losses, D_losses = [], []

for epoch in range(num_epochs):
    G.train()
    D.train()

    g_loss_total = 0
    d_loss_total = 0

    for real_data in dataloader:
        real_data = real_data.to(device)

        # Create mask to ignore NaNs
        mask = ~torch.isnan(real_data)
        real_data = torch.nan_to_num(real_data, nan=0.0)

        # === Train Discriminator ===
        z = torch.randn(real_data.size(0), latent_dim).to(device)
        fake_data = G(z).detach()

        d_real = D(real_data)
        d_fake = D(fake_data)

        real_labels = torch.ones_like(d_real)
        fake_labels = torch.zeros_like(d_fake)

        loss_D_real = criterion(d_real, real_labels)
        loss_D_fake = criterion(d_fake, fake_labels)
        loss_D = (loss_D_real + loss_D_fake) / 2

        opt_D.zero_grad()
        loss_D.backward()
        opt_D.step()

        # === Train Generator ===
        z = torch.randn(real_data.size(0), latent_dim).to(device)
        generated_data = G(z)
        d_generated = D(generated_data)

        g_loss = criterion(d_generated, torch.ones_like(d_generated))

        opt_G.zero_grad()
        g_loss.backward()
        opt_G.step()

        g_loss_total += g_loss.item()
        d_loss_total += loss_D.item()

    G_losses.append(g_loss_total / len(dataloader))
    D_losses.append(d_loss_total / len(dataloader))

    print(f"Epoch [{epoch+1}/{num_epochs}]  D_loss: {D_losses[-1]:.4f} | G_loss: {G_losses[-1]:.4f}")

# Plot training losses
plt.plot(G_losses, label="Generator Loss")
plt.plot(D_losses, label="Discriminator Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Losses")
plt.legend()
plt.grid(True)
plt.show()



```




```{python}


# Set random seed for reproducibility
np.random.seed(42)

# Parameters
num_samples = 1000

# Generate random normal data
data = np.random.randn(num_samples, input_dim)



# Create DataFrame
df_example = pd.DataFrame(data, columns=[f'feature_{i+1}' for i in range(input_dim)])

print(df_example.head())



```