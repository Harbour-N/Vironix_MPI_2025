---
title: VAE GAN framework
description: VAE with distribution analysis for each category
authors:
  - name: Nicholas Harbour, Nipuni de Silva
format:
  html:
    embed-resources: true
    code-fold: true
    number-sections: true
    toc: true
    toc-depth: 3
    date: now
    date-modified: last-modified
    date-format: "MMMM DD, YYYY, HH:mm:ss"
jupyter: python3
---


```{python}

import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import Dataset, DataLoader
from torch.utils.data import Subset
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Enable anomaly detection for debugging
torch.autograd.set_detect_anomaly(True)

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')




```


# Hyperparameters and dataset

```{python}

# Hyperparameters
batch_size = 128
learning_rate = 2e-4
num_epochs = 10
latent_dim = 100


# Load in data
df_full = pd.read_excel('iimori-ckd_processed.xlsx', sheet_name = 'minimally_processed')
df_in = pd.read_excel('iimori-ckd_processed.xlsx', sheet_name = 'eGFR_increase')
df_de = pd.read_excel('iimori-ckd_processed.xlsx', sheet_name = 'eGFR_decrease')


class CKDGANDataset(Dataset):
    def __init__(self, dataframe):
        # Store raw values directly
        self.X = torch.tensor(dataframe.values, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx]


dataset_full = CKDGANDataset(df_full)
data_loader_full = DataLoader(dataset_full, batch_size=batch_size, shuffle=True)

dataset_in = CKDGANDataset(df_in)
data_loader_in = DataLoader(dataset_in, batch_size=batch_size, shuffle=True)

dataset_de = CKDGANDataset(df_de)
data_loader_de = DataLoader(dataset_de, batch_size=batch_size, shuffle=True)





```


# Define the VAE-GAN architecture

```{python}

# Encoder
class Encoder(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 512),
            nn.ReLU(inplace=False),
            nn.Linear(512, 256),
            nn.ReLU(inplace=False),
        )
        self.fc_mu = nn.Linear(256, latent_dim)
        self.fc_logvar = nn.Linear(256, latent_dim)

    def forward(self, x):
        h = self.model(x)
        return self.fc_mu(h), self.fc_logvar(h)

# Reparameterization
def reparameterize(mu, logvar):
    std = torch.exp(0.5 * logvar)
    eps = torch.randn_like(std)
    return mu + eps * std

# Decoder (Generator)
class Decoder(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(inplace=False),
            nn.Linear(256, 512),
            nn.ReLU(inplace=False),
            nn.Linear(512, 28*28),
            nn.Sigmoid(),
            nn.Unflatten(1, (1, 28, 28))
        )

    def forward(self, z):
        return self.model(z)

# Discriminator
class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 512),
            nn.LeakyReLU(0.2, inplace=False),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=False),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

# Instantiate models
encoder = Encoder(latent_dim).to(device)
decoder = Decoder(latent_dim).to(device)
discriminator = Discriminator().to(device)

# Optimizers
optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)
optimizer_G = optim.Adam(decoder.parameters(), lr=learning_rate)
optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)



```

# Training loop

```{python}
def train_vae_gan(
    encoder, decoder, discriminator,
    train_loader,
    optimizer_E, optimizer_G, optimizer_D,
    device,
    num_epochs=10,
    verbose=True,
    label="Full"
):
    vae_loss_hist = []
    gan_loss_hist = []
    d_loss_hist = []

    for epoch in range(num_epochs):
        encoder.train()
        decoder.train()
        discriminator.train()

        total_vae_loss = 0
        total_gan_loss = 0
        total_d_loss = 0

        for x in train_loader:
            x = x.to(device)

            # VAE loss
            mu, logvar = encoder(x)
            z = reparameterize(mu, logvar)
            x_recon = decoder(z)

            recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum') / x.size(0)
            kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)
            vae_l = recon_loss + kl_div

            optimizer_E.zero_grad()
            optimizer_G.zero_grad()
            vae_l.backward(retain_graph=True)
            optimizer_E.step()
            optimizer_G.step()

            # GAN loss
            mu, logvar = encoder(x)
            z = reparameterize(mu, logvar)
            x_recon_gan = decoder(z)

            real = torch.ones(x.size(0), 1, device=device)
            fake = torch.zeros(x.size(0), 1, device=device)

            # Discriminator step
            D_real = discriminator(x)
            D_fake = discriminator(x_recon_gan.detach())
            D_loss_real = F.binary_cross_entropy(D_real, real)
            D_loss_fake = F.binary_cross_entropy(D_fake, fake)
            D_loss = D_loss_real + D_loss_fake

            optimizer_D.zero_grad()
            D_loss.backward()
            optimizer_D.step()

            # Generator step
            G_fake = discriminator(x_recon_gan)
            G_loss = F.binary_cross_entropy(G_fake, real)

            optimizer_G.zero_grad()
            G_loss.backward()
            optimizer_G.step()

            total_vae_loss += vae_l.item()
            total_gan_loss += G_loss.item()
            total_d_loss += D_loss.item()

        # Save losses
        avg_vae = total_vae_loss / len(train_loader)
        avg_gan = total_gan_loss / len(train_loader)
        avg_d = total_d_loss / len(train_loader)

        vae_loss_hist.append(avg_vae)
        gan_loss_hist.append(avg_gan)
        d_loss_hist.append(avg_d)

        if verbose:
            print(f"[{label}] Epoch [{epoch+1}/{num_epochs}] "
                  f"VAE Loss: {avg_vae:.4f} | GAN G Loss: {avg_gan:.4f} | D Loss: {avg_d:.4f}")

    return vae_loss_hist, gan_loss_hist, d_loss_hist


def plot_vae_gan_losses(vae_loss, gan_loss, d_loss, title="Training Loss Curves"):
    epochs = range(1, len(vae_loss) + 1)
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, vae_loss, label="VAE Loss")
    plt.plot(epochs, gan_loss, label="Generator Loss (G)")
    plt.plot(epochs, d_loss, label="Discriminator Loss (D)")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

```


```{python}

vae_loss_hist, gan_loss_hist, d_loss_hist = train_vae_gan(
    encoder=encoder,
    decoder=decoder,
    discriminator=discriminator,
    train_loader=data_loader_full,
    optimizer_E=optimizer_E,
    optimizer_G=optimizer_G,
    optimizer_D=optimizer_D,
    device=device,
    num_epochs=num_epochs,
    label="Full Training"
)


plot_vae_gan_losses(
    vae_loss=vae_loss_hist,
    gan_loss=gan_loss_hist,
    d_loss=d_loss_hist,
    title="VAE-GAN Training Loss Curves"
)

```


# Evaluation and sample generation

```{python}
# Generate samples
decoder.eval()
with torch.no_grad():
    z = torch.randn(10, latent_dim).to(device)
    samples = decoder(z).cpu().numpy()

fig, axes = plt.subplots(1, 10, figsize=(15, 2))
for i in range(10):
    axes[i].imshow(samples[i].squeeze(), cmap='gray')
    axes[i].axis('off')
plt.suptitle('Full VAE-GAN Generated Samples')
plt.show()



```

