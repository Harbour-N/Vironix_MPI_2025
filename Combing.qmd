---
title: Combinig biased estimators
description: balalalbadldb
authors:
  - name: Nicholas Harbour
format: 
  html:
    embed-resources: true
    code-fold: true
    number-sections: true
    toc: true
    toc-depth: 3
    date: now
    date-modified: last-modified
    date-format: "MMMM DD, YYYY, HH:mm:ss"
jupyter: python3
---





# Load libraries

```{python}
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torch.utils.data import Subset
import matplotlib.pyplot as plt
import numpy as np

# Enable anomaly detection for debugging
torch.autograd.set_detect_anomaly(True)

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


```

# Hyperparameters and dataset

```{python}

# Hyperparameters
batch_size = 128
learning_rate = 2e-4
num_epochs = 10
latent_dim = 100

# FashionMNIST class names
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# Data loading and preprocessing
transform = transforms.ToTensor()
train_dataset = datasets.FashionMNIST(root='./MNIST_fashion', train=True, download=True, transform=transform)
test_dataset = datasets.FashionMNIST(root='./MNIST_fashion', train=False, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)


```




# Define the VAE-GAN architecture

```{python}

# Encoder
class Encoder(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 512),
            nn.ReLU(inplace=False),
            nn.Linear(512, 256),
            nn.ReLU(inplace=False),
        )
        self.fc_mu = nn.Linear(256, latent_dim)
        self.fc_logvar = nn.Linear(256, latent_dim)

    def forward(self, x):
        h = self.model(x)
        return self.fc_mu(h), self.fc_logvar(h)

# Reparameterization
def reparameterize(mu, logvar):
    std = torch.exp(0.5 * logvar)
    eps = torch.randn_like(std)
    return mu + eps * std

# Decoder (Generator)
class Decoder(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(inplace=False),
            nn.Linear(256, 512),
            nn.ReLU(inplace=False),
            nn.Linear(512, 28*28),
            nn.Sigmoid(),
            nn.Unflatten(1, (1, 28, 28))
        )

    def forward(self, z):
        return self.model(z)

# Discriminator
class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 512),
            nn.LeakyReLU(0.2, inplace=False),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=False),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

# Instantiate models
encoder = Encoder(latent_dim).to(device)
decoder = Decoder(latent_dim).to(device)
discriminator = Discriminator().to(device)

# Optimizers
optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)
optimizer_G = optim.Adam(decoder.parameters(), lr=learning_rate)
optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)



```

# Training loop

```{python}
def train_vae_gan(
    encoder, decoder, discriminator,
    train_loader,
    optimizer_E, optimizer_G, optimizer_D,
    device,
    num_epochs=10,
    verbose=True,
    label="Full"
):
    vae_loss_hist = []
    gan_loss_hist = []
    d_loss_hist = []

    for epoch in range(num_epochs):
        encoder.train()
        decoder.train()
        discriminator.train()

        total_vae_loss = 0
        total_gan_loss = 0
        total_d_loss = 0

        for x, _ in train_loader:
            x = x.to(device)

            # VAE loss
            mu, logvar = encoder(x)
            z = reparameterize(mu, logvar)
            x_recon = decoder(z)

            recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum') / x.size(0)
            kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)
            vae_l = recon_loss + kl_div

            optimizer_E.zero_grad()
            optimizer_G.zero_grad()
            vae_l.backward(retain_graph=True)
            optimizer_E.step()
            optimizer_G.step()

            # GAN loss
            mu, logvar = encoder(x)
            z = reparameterize(mu, logvar)
            x_recon_gan = decoder(z)

            real = torch.ones(x.size(0), 1, device=device)
            fake = torch.zeros(x.size(0), 1, device=device)

            # Discriminator step
            D_real = discriminator(x)
            D_fake = discriminator(x_recon_gan.detach())
            D_loss_real = F.binary_cross_entropy(D_real, real)
            D_loss_fake = F.binary_cross_entropy(D_fake, fake)
            D_loss = D_loss_real + D_loss_fake

            optimizer_D.zero_grad()
            D_loss.backward()
            optimizer_D.step()

            # Generator step
            G_fake = discriminator(x_recon_gan)
            G_loss = F.binary_cross_entropy(G_fake, real)

            optimizer_G.zero_grad()
            G_loss.backward()
            optimizer_G.step()

            total_vae_loss += vae_l.item()
            total_gan_loss += G_loss.item()
            total_d_loss += D_loss.item()

        # Save losses
        avg_vae = total_vae_loss / len(train_loader)
        avg_gan = total_gan_loss / len(train_loader)
        avg_d = total_d_loss / len(train_loader)

        vae_loss_hist.append(avg_vae)
        gan_loss_hist.append(avg_gan)
        d_loss_hist.append(avg_d)

        if verbose:
            print(f"[{label}] Epoch [{epoch+1}/{num_epochs}] "
                  f"VAE Loss: {avg_vae:.4f} | GAN G Loss: {avg_gan:.4f} | D Loss: {avg_d:.4f}")

    return vae_loss_hist, gan_loss_hist, d_loss_hist


def plot_vae_gan_losses(vae_loss, gan_loss, d_loss, title="Training Loss Curves"):
    epochs = range(1, len(vae_loss) + 1)
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, vae_loss, label="VAE Loss")
    plt.plot(epochs, gan_loss, label="Generator Loss (G)")
    plt.plot(epochs, d_loss, label="Discriminator Loss (D)")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

```


# Consider a subset of only sandals and boots


```{python}

# Class labels
sandal_label = 5
boot_label = 9

# Convert targets to a NumPy array for indexing
targets_np = np.array(train_dataset.targets)

# Get indices for 'Sandal' and 'Ankle boot'
sandal_indices = np.where(targets_np == sandal_label)[0]
boot_indices = np.where(targets_np == boot_label)[0]

# Combine the indices
combined_indices = np.concatenate([sandal_indices, boot_indices])

# Create a subset with only sandals and boots
sandal_boot_dataset = Subset(train_dataset, combined_indices)

# Create DataLoader
sandal_boot_loader = DataLoader(sandal_boot_dataset, batch_size=batch_size, shuffle=True)


```

plot some example images from the subset

```{python}

import random

fig, ax = plt.subplots(1, 10, figsize=(15, 3))

# Randomly select 10 unique indices
rand_indices = random.sample(range(len(sandal_boot_dataset)), 10)

for i, idx in enumerate(rand_indices):
    img, label = sandal_boot_dataset[idx]
    img = img.squeeze().numpy()
    ax[i].imshow(img, cmap='gray')
    ax[i].set_title(class_names[label])
    ax[i].axis('off')  # Optional: turn off axes for cleaner look

plt.tight_layout()
plt.show()


```


# Train the VAE-GAN on the subset


```{python}

vae_loss_sandal_boot, gan_loss_sandal_boot, d_loss_sandal_boot = train_vae_gan(
    encoder, decoder, discriminator,
    sandal_boot_loader,
    optimizer_E, optimizer_G, optimizer_D,
    device,
    num_epochs=num_epochs,
    verbose=True,
    label="Sandal-Boot"
)

```


```{python}

# Put encoder in eval mode
encoder.eval()

all_z = []
all_labels = []

with torch.no_grad():
    for x, _ in sandal_boot_loader:
        x = x.to(device)
        mu, logvar = encoder(x)
        z = reparameterize(mu, logvar)
        all_z.append(z.cpu())
        all_labels.append(_.cpu())

# Stack all embeddings and labels
z_concat = torch.cat(all_z).numpy()
labels_concat = torch.cat(all_labels).numpy()

```

perform PCA on the embeddings to visualise


```{python}

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
x_pca = pca.fit_transform(z_concat)
plt.figure(figsize=(10, 6))
scatter = plt.scatter(x_pca[:, 0], x_pca[:, 1], c=labels_concat, cmap='tab10', s=10, alpha=0.7)
plt.title('PCA of VAE-GAN Embeddings (Sandal-Boot)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(scatter, ticks=range(10), label='Label')
plt.clim(-0.5, 9.5)
plt.grid(True)
plt.show()


```