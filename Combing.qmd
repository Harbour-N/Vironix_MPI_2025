---
title: Combinig biased estimators
description: balalalbadldb
authors:
  - name: Nicholas Harbour
format: 
  html:
    embed-resources: true
    code-fold: true
    number-sections: true
    toc: true
    toc-depth: 3
    date: now
    date-modified: last-modified
    date-format: "MMMM DD, YYYY, HH:mm:ss"
jupyter: python3
---





# Load libraries

```{python}
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torch.utils.data import Subset
import matplotlib.pyplot as plt
import numpy as np
from sklearn.decomposition import PCA

# Enable anomaly detection for debugging
torch.autograd.set_detect_anomaly(True)

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


```

# Hyperparameters and dataset

```{python}

# Hyperparameters
batch_size = 128
learning_rate = 2e-4
num_epochs = 10
latent_dim = 100

# FashionMNIST class names
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# Data loading and preprocessing
transform = transforms.ToTensor()
train_dataset = datasets.FashionMNIST(root='./MNIST_fashion', train=True, download=True, transform=transform)
test_dataset = datasets.FashionMNIST(root='./MNIST_fashion', train=False, download=True, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)


```




# Define the VAE-GAN architecture

```{python}

# Encoder
class Encoder(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 512),
            nn.ReLU(inplace=False),
            nn.Linear(512, 256),
            nn.ReLU(inplace=False),
        )
        self.fc_mu = nn.Linear(256, latent_dim)
        self.fc_logvar = nn.Linear(256, latent_dim)

    def forward(self, x):
        h = self.model(x)
        return self.fc_mu(h), self.fc_logvar(h)

# Reparameterization
def reparameterize(mu, logvar):
    std = torch.exp(0.5 * logvar)
    eps = torch.randn_like(std)
    return mu + eps * std

# Decoder (Generator)
class Decoder(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(inplace=False),
            nn.Linear(256, 512),
            nn.ReLU(inplace=False),
            nn.Linear(512, 28*28),
            nn.Sigmoid(),
            nn.Unflatten(1, (1, 28, 28))
        )

    def forward(self, z):
        return self.model(z)

# Discriminator
class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 512),
            nn.LeakyReLU(0.2, inplace=False),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2, inplace=False),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

# Instantiate models
encoder = Encoder(latent_dim).to(device)
decoder = Decoder(latent_dim).to(device)
discriminator = Discriminator().to(device)

# Optimizers
optimizer_E = optim.Adam(encoder.parameters(), lr=learning_rate)
optimizer_G = optim.Adam(decoder.parameters(), lr=learning_rate)
optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)



```

# Training loop

```{python}
def train_vae_gan(
    encoder, decoder, discriminator,
    train_loader,
    optimizer_E, optimizer_G, optimizer_D,
    device,
    num_epochs=10,
    verbose=True,
    label="Full"
):
    vae_loss_hist = []
    gan_loss_hist = []
    d_loss_hist = []

    for epoch in range(num_epochs):
        encoder.train()
        decoder.train()
        discriminator.train()

        total_vae_loss = 0
        total_gan_loss = 0
        total_d_loss = 0

        for x, _ in train_loader:
            x = x.to(device)

            # VAE loss
            mu, logvar = encoder(x)
            z = reparameterize(mu, logvar)
            x_recon = decoder(z)

            recon_loss = F.binary_cross_entropy(x_recon, x, reduction='sum') / x.size(0)
            kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)
            vae_l = recon_loss + kl_div

            optimizer_E.zero_grad()
            optimizer_G.zero_grad()
            vae_l.backward(retain_graph=True)
            optimizer_E.step()
            optimizer_G.step()

            # GAN loss
            mu, logvar = encoder(x)
            z = reparameterize(mu, logvar)
            x_recon_gan = decoder(z)

            real = torch.ones(x.size(0), 1, device=device)
            fake = torch.zeros(x.size(0), 1, device=device)

            # Discriminator step
            D_real = discriminator(x)
            D_fake = discriminator(x_recon_gan.detach())
            D_loss_real = F.binary_cross_entropy(D_real, real)
            D_loss_fake = F.binary_cross_entropy(D_fake, fake)
            D_loss = D_loss_real + D_loss_fake

            optimizer_D.zero_grad()
            D_loss.backward()
            optimizer_D.step()

            # Generator step
            G_fake = discriminator(x_recon_gan)
            G_loss = F.binary_cross_entropy(G_fake, real)

            optimizer_G.zero_grad()
            G_loss.backward()
            optimizer_G.step()

            total_vae_loss += vae_l.item()
            total_gan_loss += G_loss.item()
            total_d_loss += D_loss.item()

        # Save losses
        avg_vae = total_vae_loss / len(train_loader)
        avg_gan = total_gan_loss / len(train_loader)
        avg_d = total_d_loss / len(train_loader)

        vae_loss_hist.append(avg_vae)
        gan_loss_hist.append(avg_gan)
        d_loss_hist.append(avg_d)

        if verbose:
            print(f"[{label}] Epoch [{epoch+1}/{num_epochs}] "
                  f"VAE Loss: {avg_vae:.4f} | GAN G Loss: {avg_gan:.4f} | D Loss: {avg_d:.4f}")

    return vae_loss_hist, gan_loss_hist, d_loss_hist


def plot_vae_gan_losses(vae_loss, gan_loss, d_loss, title="Training Loss Curves"):
    epochs = range(1, len(vae_loss) + 1)
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, vae_loss, label="VAE Loss")
    plt.plot(epochs, gan_loss, label="Generator Loss (G)")
    plt.plot(epochs, d_loss, label="Discriminator Loss (D)")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

```


# Consider a subset of only sandals and boots


```{python}

# Class labels
sandal_label = 5
boot_label = 9

# Convert targets to a NumPy array for indexing
targets_np = np.array(train_dataset.targets)

# Get indices for 'Sandal' and 'Ankle boot'
sandal_indices = np.where(targets_np == sandal_label)[0]
boot_indices = np.where(targets_np == boot_label)[0]

# Combine the indices
combined_indices = np.concatenate([sandal_indices, boot_indices])

# Create a subset with only sandals and boots
sandal_boot_dataset = Subset(train_dataset, combined_indices)

# Create DataLoader
sandal_boot_loader = DataLoader(sandal_boot_dataset, batch_size=batch_size, shuffle=True)


```

plot some example images from the subset

```{python}

import random

fig, ax = plt.subplots(1, 10, figsize=(15, 3))

# Randomly select 10 unique indices
rand_indices = random.sample(range(len(sandal_boot_dataset)), 10)

for i, idx in enumerate(rand_indices):
    img, label = sandal_boot_dataset[idx]
    img = img.squeeze().numpy()
    ax[i].imshow(img, cmap='gray')
    ax[i].set_title(class_names[label])
    ax[i].axis('off')  # Optional: turn off axes for cleaner look

plt.tight_layout()
plt.show()


```


# Train the VAE-GAN on the subset


```{python}

vae_loss_sandal_boot, gan_loss_sandal_boot, d_loss_sandal_boot = train_vae_gan(
    encoder, decoder, discriminator,
    sandal_boot_loader,
    optimizer_E, optimizer_G, optimizer_D,
    device,
    num_epochs=num_epochs,
    verbose=True,
    label="Sandal-Boot"
)

```


```{python}

# Put encoder in eval mode
encoder.eval()

all_z = []
all_labels = []

with torch.no_grad():
    for x, _ in sandal_boot_loader:
        x = x.to(device)
        mu, logvar = encoder(x)
        z = reparameterize(mu, logvar)
        all_z.append(z.cpu())
        all_labels.append(_.cpu())

# Stack all embeddings and labels
z_concat = torch.cat(all_z).numpy()
labels_concat = torch.cat(all_labels).numpy()

```

perform PCA on the embeddings to visualise


```{python}


pca = PCA(n_components=2)
x_pca = pca.fit_transform(z_concat)
plt.figure(figsize=(10, 6))
plt.plot(x_pca[labels_concat == sandal_label, 0], 
         x_pca[labels_concat == sandal_label, 1], 
         'o', label='Sandal', alpha=0.7, markersize=5)
plt.plot(x_pca[labels_concat == boot_label, 0],
            x_pca[labels_concat == boot_label, 1], 
            'o', label='Ankle Boot', alpha=0.7, markersize=5)
plt.title('PCA of VAE-GAN Embeddings (Sandal-Boot)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend()
plt.grid(True)
plt.show()


```


# Perform Kernel Density Estimation (KDE) on the embeddings


```{python}

from scipy.stats import gaussian_kde

# Split embeddings (z vectors) by class
z_sandal = z_concat[labels_concat == sandal_label]
z_boot = z_concat[labels_concat == boot_label]

# Fit KDEs on full latent space (latent_dim-dimensional)
kde_sandal = gaussian_kde(z_sandal.T)  # Note: needs shape (D, N)
kde_boot = gaussian_kde(z_boot.T)

```

```{python}

# Example latent point
z_example = z_sandal[0]  # Or generate a new point

# Evaluate log-probability under each KDE
log_p_sandal = kde_sandal.logpdf(z_example)
log_p_boot = kde_boot.logpdf(z_example)

print(f"Log P(sandal): {log_p_sandal.item():.2f}")
print(f"Log P(boot):   {log_p_boot.item():.2f}")


```


```{python}

# 2D slice through two latent dims
from matplotlib import cm

i, j = 0, 1  # select latent dims to visualize
xmin, xmax = z_concat[:, i].min(), z_concat[:, i].max()
ymin, ymax = z_concat[:, j].min(), z_concat[:, j].max()
xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]
grid = np.vstack([xx.ravel(), yy.ravel()])

# Build extended grid with D dimensions, rest fixed at zero
grid_d = np.zeros((latent_dim, grid.shape[1]))
grid_d[i, :] = grid[0]
grid_d[j, :] = grid[1]

# Evaluate both KDEs
zz_sandal = kde_sandal(grid_d).reshape(xx.shape)
zz_boot = kde_boot(grid_d).reshape(xx.shape)

# Plot comparison
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.contourf(xx, yy, zz_sandal, cmap='Reds')
plt.title(f"KDE (Sandal) on latent dims {i}-{j}")
plt.xlabel(f"z[{i}]")
plt.ylabel(f"z[{j}]")

plt.subplot(1, 2, 2)
plt.contourf(xx, yy, zz_boot, cmap='Blues')
plt.title(f"KDE (Boot) on latent dims {i}-{j}")
plt.xlabel(f"z[{i}]")
plt.ylabel(f"z[{j}]")

plt.tight_layout()
plt.show()


```


# Sample a new point in latent space and caluclate the weights


```{python}

def calc_weights(point, kde_sandal, kde_boot):
    """
    Calculate the weights for a new point based on KDEs.
    """
    log_p_sandal = kde_sandal.logpdf(point)
    log_p_boot = kde_boot.logpdf(point)
    
    # Convert log probabilities to probabilities
    p_sandal = np.exp(log_p_sandal)
    p_boot = np.exp(log_p_boot)
    
    # Normalize to get weights
    total = p_sandal + p_boot
    weight_sandal = p_sandal / total
    weight_boot = p_boot / total # this are similarity scores

    weight_sandal = 1 - weight_sandal
    weight_boot = 1 - weight_boot
    
    return weight_sandal, weight_boot



```


```{python}

# Generate 3 random points that are in different regions of latent space
# Compute mean latent vectors
mean_sandal = torch.tensor(z_sandal.mean(axis=0), dtype=torch.float32).to(device)
mean_boot = torch.tensor(z_boot.mean(axis=0), dtype=torch.float32).to(device) * torch.randn_like(mean_boot)*20

# Interpolate between them
middle_point = 0.5 * ( torch.tensor(z_sandal.mean(axis=0), dtype=torch.float32).to(device) +  torch.tensor(z_boot.mean(axis=0), dtype=torch.float32).to(device))

# Now you have 3 meaningful latent points
new_point_1 = mean_sandal  # black
new_point_2 = mean_boot    # Green
new_point_3 = middle_point # Pink


# Calulate the weights
weight_1 = calc_weights(new_point_1.cpu().numpy(), kde_sandal, kde_boot)
weight_2 = calc_weights(new_point_2.cpu().numpy(), kde_sandal, kde_boot)
weight_3 = calc_weights(new_point_3.cpu().numpy(), kde_sandal, kde_boot)

print(f"New Point 1 Weights: {weight_1}")
print(f"New Point 2 Weights: {weight_2}")
print(f"New Point 3 Weights: {weight_3}")


plt.figure(figsize=(10, 6))
plt.plot(x_pca[labels_concat == sandal_label, 0], 
         x_pca[labels_concat == sandal_label, 1], 
         'o', label='Sandal', alpha=0.1, markersize=5)
plt.plot(x_pca[labels_concat == boot_label, 0],
            x_pca[labels_concat == boot_label, 1], 
            'o', label='Ankle Boot', alpha=0.1, markersize=5)
plt.plot(new_point_1[0].item(), new_point_1[1].item(), 
         'x', color='black', markersize=10, label='New Point')
plt.plot(new_point_2[0].item(), new_point_2[1].item(), 
         'x', color='green', markersize=10)
plt.plot(new_point_3[0].item(), new_point_3[1].item(), 
         'x', color='m', markersize=10)
plt.title('PCA of VAE-GAN Embeddings (Sandal-Boot)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend()
plt.grid(True)
plt.show()


```

Calculate the weights for the new points
```{python}




```



